# collect_fulltext.py
import requests, json, time, argparse, os
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import pandas as pd

BASE_URL = "https://api.elsevier.com/content"

SEED_JOURNALS = [
    "Journal of Catalysis",
    "ACS Catalysis",
    "Applied Catalysis B: Environmental",
    "Applied Catalysis A: General",
    "Catalysis Today",
    "Catalysis Science & Technology",
    "ChemCatChem",
    "Chinese Journal of Catalysis",
    "Catalysis Communications",
    "Molecular Catalysis",
    "Journal of Energy Chemistry",
]

def make_headers(api_key: str, inst_token: Optional[str]) -> Dict[str, str]:
    h = {
        "X-ELS-APIKey": api_key,
        "Accept": "application/json",
        "Content-Type": "application/json",
    }
    if inst_token:
        h["X-ELS-Insttoken"] = inst_token
    return h

def sd_search(api_key: str, inst_token: Optional[str], search_query: str, start: int=0, count: int=25) -> Optional[Dict]:
    """
    ScienceDirect ê²€ìƒ‰. ê³„ì •/ì—”ë“œí¬ì¸íŠ¸ ì°¨ì´ë¥¼ ëŒ€ë¹„í•´ query/qs ë‘ ë°©ì‹ ìë™ í´ë°±.
    """
    headers = make_headers(api_key, inst_token)
    url = f"{BASE_URL}/search/sciencedirect"
    attempts = [
        {"params": {"query": search_query, "start": start, "count": count}},
        {"params": {"qs": search_query, "offset": start, "count": count}},
    ]
    for attempt in attempts:
        try:
            r = requests.get(url, headers=headers, params=attempt["params"], timeout=30)
            if r.status_code == 200:
                data = r.json()
                entries = data.get("search-results", {}).get("entry", [])
                if entries and not (len(entries) == 1 and "error" in entries[0]):
                    return data
            elif r.status_code in (403, 429):
                # ê¶Œí•œ/ì¿¼í„°: ì•½í•œ ë°±ì˜¤í”„ í›„ ë‹¤ìŒ ì‹œë„
                time.sleep(2)
        except requests.RequestException:
            pass
    return None

def get_total_results(api_key: str, inst_token: Optional[str], keyword: str, journal: Optional[str]=None) -> int:
    q = f'{keyword} AND SRCTITLE("{journal}")' if journal else keyword
    data = sd_search(api_key, inst_token, q, start=0, count=1)
    if not data:
        return 0
    return int(data.get("search-results", {}).get("opensearch:totalResults", "0"))

def discover_catalysis_journals(api_key: str, inst_token: Optional[str], sample_count: int=200) -> List[str]:
    """
    'catalysis'ë¡œ SD ê²€ìƒ‰ â†’ publicationName ì¤‘ 'catalysis' í¬í•¨ ì €ë„ ì¶”ì¶œ + ì‹œë“œ í•©ì¹˜ê¸°.
    """
    found = set(SEED_JOURNALS)
    data = sd_search(api_key, inst_token, "catalysis", start=0, count=sample_count)
    if data:
        entries = data.get("search-results", {}).get("entry", [])
        for e in entries:
            j = e.get("prism:publicationName", "")
            if j and "catalysis" in j.lower():
                found.add(j)
    return sorted(found)

def extract_pii_from_uri(uri: str) -> Optional[str]:
    if not uri:
        return None
    if "/pii/" in uri:
        return uri.split("/pii/")[-1] or None
    return None

def extract_authors(entry: Dict) -> str:
    a = entry.get("authors")
    if isinstance(a, dict) and "author" in a:
        authors = a["author"]
        if isinstance(authors, list):
            return "; ".join([
                f"{x.get('given-name','')} {x.get('surname','')}".strip()
                for x in authors if isinstance(x, dict)
            ])
        if isinstance(authors, dict):
            return f"{authors.get('given-name','')} {authors.get('surname','')}".strip()
    creator = entry.get("dc:creator")
    if isinstance(creator, str):
        return creator
    if isinstance(creator, list):
        return "; ".join([str(c) for c in creator])
    return ""

def get_full_text_json(api_key: str, inst_token: Optional[str], pii: str, max_retries: int=4) -> Optional[Dict]:
    """
    ì „ë¬¸ JSON(FULL) ì¡°íšŒ. 403/404/429 ì²˜ë¦¬ + ì§€ìˆ˜ ë°±ì˜¤í”„.
    """
    headers = make_headers(api_key, inst_token)
    url = f"{BASE_URL}/article/pii/{pii}"
    params = {"view": "FULL"}
    delay = 1.5
    for attempt in range(1, max_retries+1):
        try:
            r = requests.get(url, headers=headers, params=params, timeout=45)
            if r.status_code == 200:
                return r.json()
            elif r.status_code == 404:
                # ì—†ëŠ” PII
                return None
            elif r.status_code in (403, 429, 500, 502, 503, 504):
                time.sleep(delay)
                delay *= 2
            else:
                # ê¸°íƒ€ ì—ëŸ¬
                time.sleep(delay)
                delay *= 1.5
        except requests.RequestException:
            time.sleep(delay)
            delay *= 2
    return None

def collect_from_journal(api_key: str, inst_token: Optional[str], keyword: str, journal: str, need: int,
                         out_dir: str, start_index: int=0, page_size: int=25,
                         seen: Optional[set]=None) -> Tuple[int, int]:
    """
    ë‹¨ì¼ ì €ë„ì—ì„œ keywordë¡œ needê°œê¹Œì§€ ìˆ˜ì§‘ + ì „ë¬¸ ì €ì¥.
    ë°˜í™˜: (ì €ì¥ ì„±ê³µ ê°œìˆ˜, ë‹¤ìŒ start ì¸ë±ìŠ¤)
    """
    os.makedirs(out_dir, exist_ok=True)
    saved = 0
    start = start_index
    seen = seen or set()

    while saved < need:
        q = f'{keyword} AND SRCTITLE("{journal}")'
        data = sd_search(api_key, inst_token, q, start=start, count=min(page_size, need - saved))
        if not data:
            break
        entries = data.get("search-results", {}).get("entry", [])
        if not entries or (len(entries) == 1 and "error" in entries[0]):
            break

        for e in entries:
            if "error" in e:
                continue
            uri = e.get("prism:url", "")
            doi = e.get("prism:doi", "")
            pii = extract_pii_from_uri(uri)
            identifier = doi or pii or e.get("dc:title", "")
            if not identifier or identifier in seen:
                continue
            seen.add(identifier)

            meta = {
                "title": e.get("dc:title", ""),
                "authors": extract_authors(e),
                "journal": e.get("prism:publicationName", ""),
                "date": e.get("prism:coverDate", ""),
                "doi": doi,
                "abstract": e.get("dc:description", ""),
                "uri": uri,
                "pii": pii,
            }

            if not pii:
                # PII ì—†ìœ¼ë©´ ì „ë¬¸ ì ‘ê·¼ ë¶ˆê°€ â†’ ìŠ¤í‚µ(ìš”êµ¬ì‚¬í•­: ë¬´ì¡°ê±´ ì „ë¬¸ ì €ì¥)
                continue

            full_json = get_full_text_json(api_key, inst_token, pii)
            if not full_json:
                # ì ‘ê·¼ ì‹¤íŒ¨/ê¶Œí•œ/404 â†’ ìŠ¤í‚µ
                continue

            # ì €ì¥ íŒŒì¼ëª…: PII ê¸°ì¤€
            safe_pii = "".join(c for c in pii if c.isalnum())
            out_path = os.path.join(out_dir, f"{safe_pii}.json")
            payload = {
                "metadata": meta,
                "fulltext": full_json
            }
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            saved += 1

            if saved >= need:
                break

        start += page_size
        time.sleep(0.5)  # rate-limit ì—¬ìœ 
    return saved, start

def save_csv(rows: List[Dict], path: str):
    df = pd.DataFrame([{k: v for k, v in r.items()} for r in rows])
    df.to_csv(path, index=False, encoding="utf-8")
    print(f"ğŸ’¾ Saved: {path} ({len(rows)} rows)")

def load_existing_papers(out_dir: str) -> Tuple[set, int]:
    """
    ê¸°ì¡´ ì €ì¥ëœ JSON íŒŒì¼ì—ì„œ PII/DOI ì¶”ì¶œí•´ì„œ seen ì„¸íŠ¸ êµ¬ì„± + ê°œìˆ˜ ì¹´ìš´íŠ¸
    """
    seen = set()
    count = 0
    
    if not os.path.exists(out_dir):
        return seen, count
    
    for filename in os.listdir(out_dir):
        if not filename.endswith('.json') or filename == 'run_log.json':
            continue
        
        try:
            filepath = os.path.join(out_dir, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                meta = data.get('metadata', {})
                
                # ì‹ë³„ì ì¶”ì¶œ (collect_from_journalê³¼ ë™ì¼í•œ ìš°ì„ ìˆœìœ„)
                doi = meta.get('doi', '')
                pii = meta.get('pii', '')
                title = meta.get('title', '')
                identifier = doi or pii or title
                
                if identifier:
                    seen.add(identifier)
                    count += 1
        except (json.JSONDecodeError, IOError):
            # ì†ìƒëœ íŒŒì¼ì€ ìŠ¤í‚µ
            continue
    
    return seen, count


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--api_key", required=True, help="Elsevier API key")
    ap.add_argument("--inst_token", default=None, help="Institution token (optional, ê¶Œí•œ ë¬¸ì œì‹œ ê¶Œì¥)")
    ap.add_argument("--keyword", default="catalyst")
    ap.add_argument("--target", type=int, default=10000)
    ap.add_argument("--page_size", type=int, default=25)
    ap.add_argument("--out_dir", default="sd_fulltexts", help="ì „ë¬¸ JSON ì €ì¥ ë””ë ‰í† ë¦¬")
    ap.add_argument("--dry_discover_only", action="store_true")
    args = ap.parse_args()

    # ===== ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì¶”ê°€ =====
    print("=== Step 0. ê¸°ì¡´ ì €ì¥ëœ ë…¼ë¬¸ í™•ì¸ ===")
    seen, saved_total = load_existing_papers(args.out_dir)
    print(f"ê¸°ì¡´ ì €ì¥ëœ ë…¼ë¬¸: {saved_total}ê°œ")
    print(f"ì¤‘ë³µ ì²´í¬ìš© ì‹ë³„ì: {len(seen)}ê°œ")
    
    if saved_total >= args.target:
        print(f"âœ… ì´ë¯¸ ëª©í‘œ({args.target})ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        return
    
    need = args.target - saved_total
    print(f"ì¶”ê°€ë¡œ ìˆ˜ì§‘í•  ë…¼ë¬¸: {need}ê°œ\n")
    # ================================

    print("=== Step 1. ê´€ë ¨ ì €ë„ ìë™ íƒìƒ‰ ===")
    journals = discover_catalysis_journals(args.api_key, args.inst_token)
    print(f"ë°œê²¬/ì‹œë“œ ì €ë„ ìˆ˜: {len(journals)}")
    for j in journals:
        print(" -", j)

    print("\n=== Step 2. ì €ë„ë³„ 'catalysis' ê°€ëŠ¥ í¸ìˆ˜ ì§‘ê³„ ===")
    counts = []
    total_available = 0
    for j in journals:
        cnt = get_total_results(args.api_key, args.inst_token, args.keyword, j)
        counts.append((j, cnt))
        total_available += cnt
        print(f"{j}: {cnt}")
    counts.sort(key=lambda x: x[1], reverse=True)
    print(f"\nì´ ê°€ëŠ¥ í¸ìˆ˜: {total_available}")

    if args.dry_discover_only:
        print("íƒìƒ‰ ì „ìš© ëª¨ë“œ ì¢…ë£Œ.")
        return

    if total_available <= 0:
        print("ìˆ˜ì§‘ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        return

    os.makedirs(args.out_dir, exist_ok=True)
    meta_rows: List[Dict] = []

    print("\n=== Step 3. ìˆ˜ì§‘(ì „ë¬¸ ì €ì¥) ì‹œì‘ ===")
    for j, cnt in counts:
        if need <= 0 or cnt == 0:
            continue
        
        to_aim = min(cnt, need * 2)
        print(f"\n[{j}] ì‹œë„ ëª©í‘œ(ë©”íƒ€ ì¡°íšŒ): {to_aim} / ë‚¨ì€ ì „ë¬¸ ì €ì¥ í•„ìš”: {need}")

        saved_now, _ = collect_from_journal(
            api_key=args.api_key,
            inst_token=args.inst_token,
            keyword=args.keyword,
            journal=j,
            need=min(need, to_aim),
            out_dir=args.out_dir,
            start_index=0,
            page_size=args.page_size,
            seen=seen  # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë¡œë“œí•œ seen ì‚¬ìš©
        )
        saved_total += saved_now
        need -= saved_now
        print(f"  â†’ ì „ë¬¸ ì €ì¥ ì„±ê³µ: {saved_now}ê°œ (ëˆ„ì  {saved_total})")

        if saved_total >= args.target:
            break
        time.sleep(1.0)

    print(f"\nìµœì¢… ì „ë¬¸ ì €ì¥ ê°œìˆ˜: {saved_total} / ëª©í‘œ {args.target}")
    if saved_total < args.target:
        print("âš ï¸ ê¸°ê´€ ê¶Œí•œ/ì œí•œìœ¼ë¡œ ì „ë¬¸ ì €ì¥ì´ ëª©í‘œì— ë¯¸ë‹¬í–ˆìŠµë‹ˆë‹¤. ìº í¼ìŠ¤ ë„¤íŠ¸ì›Œí¬/í”„ë¡ì‹œ ë˜ëŠ” Insttoken ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

    log = {
        "timestamp": datetime.now().isoformat(),
        "keyword": args.keyword,
        "target": args.target,
        "saved_fulltexts": saved_total,
        "out_dir": args.out_dir,
        "journals_tried": [j for j, _ in counts]
    }
    with open(os.path.join(args.out_dir, "run_log.json"), "w", encoding="utf-8") as f:
        json.dump(log, f, ensure_ascii=False, indent=2)
    print("ì™„ë£Œ!")

if __name__ == "__main__":
    main()
