{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM4LRdVlHT5tYhSjC1l4nPB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hsKyUtuK1xfz","executionInfo":{"status":"ok","timestamp":1766290751897,"user_tz":-540,"elapsed":17615,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"c677e632-6a81-404b-c84e-15e75ed41ad8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/AItom/safety_embedding_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLhCiNul2HBX","executionInfo":{"status":"ok","timestamp":1766290752458,"user_tz":-540,"elapsed":560,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"640b0859-b2cc-410b-fd48-df639e82477d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AItom/safety_embedding_model\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"O15pwTYj07Lb","executionInfo":{"status":"ok","timestamp":1766290765926,"user_tz":-540,"elapsed":10845,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","from pathlib import Path\n","from typing import List, Tuple, Optional\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score,\n","    f1_score, roc_auc_score, confusion_matrix, classification_report\n",")\n","from tqdm import tqdm\n","\n","from utils.cas_to_formula import cas_to_formula\n","from utils.multi_property_embedding import get_combined_embedding, DEFAULT_PROPERTIES\n"]},{"cell_type":"code","source":["class RiskDataset(Dataset):\n","    \"\"\"위험성 분류를 위한 데이터셋\"\"\"\n","\n","    def __init__(self, formulas: List[str] = None, labels: np.ndarray = None,\n","                 embeddings: torch.Tensor = None, use_embeddings: bool = False):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        formulas : List[str], optional\n","            화학식 리스트 (use_embeddings=False일 때 사용)\n","        labels : np.ndarray, optional\n","            라벨 배열 (N,) - 0 또는 1\n","        embeddings : torch.Tensor, optional\n","            임베딩 벡터 텐서 (N, embedding_dim) (use_embeddings=True일 때 사용)\n","        use_embeddings : bool\n","            True면 embeddings를 직접 사용, False면 formulas에서 임베딩 추출\n","        \"\"\"\n","        self.use_embeddings = use_embeddings\n","\n","        if use_embeddings:\n","            # 임베딩을 직접 사용하는 경우\n","            if embeddings is None or labels is None:\n","                raise ValueError(\"use_embeddings=True일 때 embeddings와 labels가 필요합니다.\")\n","            self.embeddings = embeddings\n","            self.labels = torch.LongTensor(labels) if isinstance(labels, np.ndarray) else labels\n","            self.formulas = None  # 필요시 저장 가능\n","        else:\n","            # 기존 방식: formulas에서 임베딩 추출\n","            if formulas is None or labels is None:\n","                raise ValueError(\"use_embeddings=False일 때 formulas와 labels가 필요합니다.\")\n","            self.formulas = formulas\n","            self.labels = torch.LongTensor(labels) if isinstance(labels, np.ndarray) else labels\n","            self.embeddings = None\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        if self.use_embeddings:\n","            return self.embeddings[idx], self.labels[idx]\n","        else:\n","            return self.formulas[idx], self.labels[idx]"],"metadata":{"id":"lUTUcb2G1cD_","executionInfo":{"status":"ok","timestamp":1766290766845,"user_tz":-540,"elapsed":17,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class RiskClassifier(nn.Module):\n","    \"\"\"위험성 이진 분류를 위한 MLP 모델\"\"\"\n","\n","    def __init__(\n","        self,\n","        properties: List[str] = None,\n","        input_dim: int = 84,\n","        hidden_dims: List[int] = [256, 128, 64, 32],\n","        dropout_rate: float = 0,\n","        compute_device: Optional[str] = None,\n","        use_embeddings: bool = False,\n","    ):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        properties : List[str]\n","            사용할 property 리스트 (임베딩 차원 결정)\n","        hidden_dims : List[int]\n","            은닉층 차원 리스트\n","        dropout_rate : float\n","            Dropout 비율\n","        compute_device : str, optional\n","            계산 디바이스 (CrabNet 임베딩 추출용)\n","        \"\"\"\n","        super(RiskClassifier, self).__init__()\n","\n","        self.properties = properties\n","        self.compute_device = compute_device\n","        self.use_embeddings = use_embeddings\n","        self.input_dim = input_dim\n","\n","\n","        layers = []\n","        prev_dim = self.input_dim\n","\n","        for hidden_dim in hidden_dims:\n","            layers.append(nn.Linear(prev_dim, hidden_dim))\n","            layers.append(nn.SiLU())\n","            layers.append(nn.BatchNorm1d(hidden_dim))\n","            layers.append(nn.Dropout(dropout_rate))\n","            prev_dim = hidden_dim\n","\n","        # 출력층 (이진 분류)\n","        layers.append(nn.Linear(prev_dim, 2))\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, inputs) -> torch.Tensor:\n","        \"\"\"\n","        Parameters\n","        ----------\n","        inputs : torch.Tensor or List[str]\n","            - torch.Tensor: 임베딩 벡터 (batch_size, embedding_dim) - use_embeddings=True일 때\n","            - List[str]: 화학식 리스트 (배치) - use_embeddings=False일 때\n","\n","        Returns\n","        -------\n","        outputs : torch.Tensor\n","            분류 로짓 (batch_size, 2)\n","        valid_indices : torch.Tensor (optional)\n","            유효한 샘플의 인덱스 (use_embeddings=False일 때만 반환)\n","        \"\"\"\n","        # 임베딩을 직접 받는 경우\n","        if isinstance(inputs, torch.Tensor):\n","            x = inputs\n","            return self.network(x)\n","\n","        # Formula 리스트를 받는 경우 (기존 방식)\n","        formulas = inputs\n","        embeddings = []\n","        valid_indices = []\n","        failed_formulas = []\n","        with torch.no_grad():  # CrabNet 임베딩 추출 시 gradient 비활성화\n","            for idx, formula in enumerate(formulas):\n","                try:\n","                    emb = get_combined_embedding(\n","                        formula=formula,\n","                        properties=self.properties,\n","                        compute_device=self.compute_device,\n","                        verbose=False,\n","                    )\n","                    # 1D 벡터를 명시적으로 확인하고 차원 보장\n","                    if emb.dim() == 1:\n","                        embeddings.append(emb)\n","                    else:\n","                        # 이미 2D인 경우 flatten\n","                        embeddings.append(emb.flatten())\n","                    valid_indices.append(idx)\n","                except Exception as e:\n","                    # 임베딩 추출 실패 시 formula 기록\n","                    failed_formulas.append((formula, str(e)))\n","                    print(f\"[ERROR] 임베딩 추출 실패 - formula: {formula}, error: {e}\")\n","\n","        # 실패한 formula가 있으면 경고\n","        if failed_formulas:\n","            print(f\"[WARNING] {len(failed_formulas)}개 formula의 임베딩 추출 실패:\")\n","            for formula, error in failed_formulas:\n","                print(f\"  - {formula}: {error}\")\n","\n","        # 유효한 임베딩이 없으면 에러\n","        if len(embeddings) == 0:\n","            raise RuntimeError(\n","                f\"모든 formula의 임베딩 추출 실패. 배치 크기: {len(formulas)}, \"\n","                f\"실패한 formula: {[f[0] for f in failed_formulas]}\"\n","            )\n","\n","        # 배치로 스택: (batch_size, embedding_dim) 형태로 변환\n","        x = torch.stack(embeddings)  # (valid_batch_size, embedding_dim)\n","\n","        # 임베딩 수와 원래 formula 수가 다르면 경고\n","        if len(embeddings) != len(formulas):\n","            print(f\"[WARNING] 배치 크기 불일치: 원래 {len(formulas)}개, 성공 {len(embeddings)}개\")\n","            print(f\"  실패한 formula: {[f[0] for f in failed_formulas]}\")\n","\n","        # 첫 번째 forward에서 실제 임베딩 차원 확인 및 MLP 재구성\n","        if not self._input_dim_initialized:\n","            actual_dim = x.shape[1]\n","            if actual_dim != self.input_dim:\n","                self.input_dim = actual_dim\n","                if hasattr(self, 'network') and len(self.network) > 0:\n","                    first_layer = self.network[0]\n","                    if isinstance(first_layer, nn.Linear) and first_layer.in_features != actual_dim:\n","                        print(f\"Warning: Input dimension mismatch. Expected {first_layer.in_features}, got {actual_dim}\")\n","                        print(f\"Please ensure the model is initialized with correct input_dim or recreate the model.\")\n","            self._input_dim_initialized = True\n","\n","        x = x.detach()  # CrabNet gradient 완전 차단\n","\n","        # MLP 통과 (MLP만 학습됨)\n","        outputs = self.network(x)\n","\n","        # 유효한 인덱스를 텐서로 변환\n","        valid_indices_tensor = torch.tensor(valid_indices, dtype=torch.long)\n","\n","        return outputs, valid_indices_tensor"],"metadata":{"id":"Cg4M1KED2bd_","executionInfo":{"status":"ok","timestamp":1766290770534,"user_tz":-540,"elapsed":2,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, dataloader, criterion, optimizer, device, epoch=None, use_embeddings=False):\n","    \"\"\"한 에폭 훈련\"\"\"\n","    model.train()\n","    total_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","    processed_batches = 0\n","\n","    # tqdm으로 진행 바 표시\n","    desc = f\"Epoch {epoch}\" if epoch is not None else \"Training\"\n","    pbar = tqdm(dataloader, desc=desc, leave=False)\n","\n","    for batch_data in pbar:\n","        if use_embeddings:\n","            # embeddings를 직접 사용하는 경우\n","            embeddings, labels = batch_data\n","            labels = labels.to(device)\n","            embeddings = embeddings.to(device)\n","\n","            optimizer.zero_grad()\n","            try:\n","                outputs = model(embeddings)\n","                outputs = outputs.to(device)\n","\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                processed_batches += 1\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                # 진행 바 업데이트\n","                current_loss = total_loss / processed_batches\n","                current_acc = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0.0\n","                pbar.set_postfix({\n","                    'loss': f'{current_loss:.4f}',\n","                    'acc': f'{current_acc:.4f}',\n","                    'processed': processed_batches\n","                })\n","            except Exception as e:\n","                pbar.set_postfix({'status': f'error: {str(e)[:30]}'})\n","                print(f\"[ERROR] 모델 forward 실패\")\n","                print(f\"  에러: {e}\")\n","                continue\n","        else:\n","            # 기존 방식: formulas에서 임베딩 추출\n","            formulas, labels = batch_data\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            # 모델이 formula 리스트를 받아서 임베딩 추출 후 분류\n","            try:\n","                outputs, valid_indices = model(formulas)\n","                outputs = outputs.to(device)\n","                valid_indices = valid_indices.to(device)\n","\n","                # 유효한 샘플의 labels만 필터링\n","                valid_labels = labels[valid_indices]\n","\n","                # 유효한 샘플이 없으면 스킵\n","                if len(valid_labels) == 0:\n","                    pbar.set_postfix({'status': 'skipped (no valid samples)'})\n","                    continue\n","\n","                loss = criterion(outputs, valid_labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                processed_batches += 1\n","                _, preds = torch.max(outputs, 1)\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(valid_labels.cpu().numpy())\n","\n","                # 진행 바 업데이트 (현재 loss와 accuracy 표시)\n","                current_loss = total_loss / processed_batches\n","                current_acc = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0.0\n","                pbar.set_postfix({\n","                    'loss': f'{current_loss:.4f}',\n","                    'acc': f'{current_acc:.4f}',\n","                    'processed': processed_batches\n","                })\n","            except Exception as e:\n","                pbar.set_postfix({'status': f'error: {str(e)[:30]}'})\n","                print(f\"[ERROR] 모델 forward 실패 - formula들: {formulas}\")\n","                print(f\"  에러: {e}\")\n","                continue\n","\n","    pbar.close()\n","\n","    if processed_batches == 0:\n","        avg_loss = 0.0\n","        accuracy = 0.0\n","    else:\n","        avg_loss = total_loss / processed_batches\n","        accuracy = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0.0\n","\n","    return avg_loss, accuracy\n","\n","\n","def evaluate(model, dataloader, criterion, device, use_embeddings=False):\n","    \"\"\"모델 평가\"\"\"\n","    model.eval()\n","    total_loss = 0.0\n","    all_preds = []\n","    all_probs = []\n","    all_labels = []\n","    processed_batches = 0\n","\n","    with torch.no_grad():\n","        for batch_data in dataloader:\n","            if use_embeddings:\n","                # embeddings를 직접 사용하는 경우\n","                embeddings, labels = batch_data\n","                labels = labels.to(device)\n","                embeddings = embeddings.to(device)\n","\n","                try:\n","                    outputs = model(embeddings)\n","                    outputs = outputs.to(device)\n","\n","                    loss = criterion(outputs, labels)\n","\n","                    total_loss += loss.item()\n","                    processed_batches += 1\n","                    probs = torch.softmax(outputs, dim=1)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    all_preds.extend(preds.cpu().numpy())\n","                    all_probs.extend(probs[:, 1].cpu().numpy())\n","                    all_labels.extend(labels.cpu().numpy())\n","                except Exception as e:\n","                    print(f\"[ERROR] 모델 forward 실패\")\n","                    print(f\"  에러: {e}\")\n","                    continue\n","            else:\n","                # 기존 방식: formulas에서 임베딩 추출\n","                formulas, labels = batch_data\n","                labels = labels.to(device)\n","\n","                # 모델이 formula 리스트를 받아서 임베딩 추출 후 분류\n","                try:\n","                    outputs, valid_indices = model(formulas)\n","                    outputs = outputs.to(device)\n","                    valid_indices = valid_indices.to(device)\n","\n","                    # 유효한 샘플의 labels만 필터링\n","                    valid_labels = labels[valid_indices]\n","\n","                    # 유효한 샘플이 없으면 스킵\n","                    if len(valid_labels) == 0:\n","                        print(f\"[WARNING] 배치 내 유효한 샘플이 없음 - formula들: {formulas}\")\n","                        continue\n","\n","                    loss = criterion(outputs, valid_labels)\n","\n","                    total_loss += loss.item()\n","                    processed_batches += 1\n","                    probs = torch.softmax(outputs, dim=1)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    all_preds.extend(preds.cpu().numpy())\n","                    all_probs.extend(probs[:, 1].cpu().numpy())  # 위험 클래스 확률\n","                    all_labels.extend(valid_labels.cpu().numpy())\n","                except Exception as e:\n","                    print(f\"[ERROR] 모델 forward 실패 - formula들: {formulas}\")\n","                    print(f\"  에러: {e}\")\n","                    continue\n","\n","    if processed_batches == 0:\n","        avg_loss = 0.0\n","        accuracy = 0.0\n","        precision = 0.0\n","        recall = 0.0\n","        f1 = 0.0\n","        auc = 0.0\n","    else:\n","        avg_loss = total_loss / processed_batches\n","        if len(all_labels) == 0:\n","            accuracy = 0.0\n","            precision = 0.0\n","            recall = 0.0\n","            f1 = 0.0\n","            auc = 0.0\n","        else:\n","            accuracy = accuracy_score(all_labels, all_preds)\n","            precision = precision_score(all_labels, all_preds, zero_division=0)\n","            recall = recall_score(all_labels, all_preds, zero_division=0)\n","            f1 = f1_score(all_labels, all_preds, zero_division=0)\n","            try:\n","                auc = roc_auc_score(all_labels, all_probs)\n","            except ValueError:\n","                auc = 0.0\n","\n","    return {\n","        'loss': avg_loss,\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'auc': auc,\n","        'predictions': all_preds,\n","        'probabilities': all_probs,\n","        'labels': all_labels,\n","    }"],"metadata":{"id":"akhZM4Tl2nnZ","executionInfo":{"status":"ok","timestamp":1766290773196,"user_tz":-540,"elapsed":3,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 랜덤 시드 설정\n","torch.manual_seed(45)\n","np.random.seed(45)\n","\n","# 디바이스 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"사용 디바이스: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQ5EhZko4rhX","executionInfo":{"status":"ok","timestamp":1766290776906,"user_tz":-540,"elapsed":23,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"b99fd4c1-b98a-4ab2-8367-2f0560c1fa07"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["사용 디바이스: cpu\n"]}]},{"cell_type":"code","source":["embeddings_path1 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings4.pt'\n","embeddings_path2 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings5.pt'\n","embeddings_path3 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings6.pt'\n","embeddings_path4 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings7.pt'\n","embeddings_path5 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings8.pt'\n","embeddings_path6 = '/content/drive/MyDrive/AItom/safety_embedding_model/training_data/embeddings9.pt'\n","\n","embeddings_data1 = torch.load(embeddings_path1)\n","embeddings_data2 = torch.load(embeddings_path2)\n","embeddings_data3 = torch.load(embeddings_path3)\n","embeddings_data4 = torch.load(embeddings_path4)\n","embeddings_data5 = torch.load(embeddings_path5)\n","embeddings_data6 = torch.load(embeddings_path6)\n","\n","embeddings1 = embeddings_data1['embeddings']\n","labels1 = embeddings_data1['labels']  # (N,)\n","formulas = embeddings_data1.get('formulas', [])  # List[str] (optional)\n","properties = embeddings_data1.get('properties', DEFAULT_PROPERTIES)\n","\n","embeddings2 = embeddings_data2['embeddings']\n","labels2 = embeddings_data2['labels']  # (N,)\n","formulas2 = embeddings_data2.get('formulas', [])  # List[str] (optional)\n","\n","embeddings3 = embeddings_data3['embeddings']\n","labels3 = embeddings_data3['labels']  # (N,)\n","formulas3 = embeddings_data3.get('formulas', [])  # List[str] (optional)\n","\n","embeddings4 = embeddings_data4['embeddings']\n","labels4 = embeddings_data4['labels']  # (N,)\n","formulas4 = embeddings_data4.get('formulas', [])  # List[str] (optional)\n","\n","embeddings5 = embeddings_data5['embeddings']\n","labels5 = embeddings_data5['labels']  # (N,)\n","formulas5 = embeddings_data5.get('formulas', [])  # List[str] (optional)\n","\n","embeddings6 = embeddings_data6['embeddings']\n","labels6 = embeddings_data6['labels']  # (N,)\n","formulas6 = embeddings_data6.get('formulas', [])  # List[str] (optional)\n","\n","embeddings = torch.cat(\n","    [embeddings1, embeddings2, embeddings3,\n","     embeddings4, embeddings5, embeddings6],\n","    dim=0\n",")\n","\n","labels = torch.cat([labels1, labels2, labels3, labels4, labels5, labels6], dim=0)\n","formulas.extend(formulas2)\n","formulas.extend(formulas3)\n","formulas.extend(formulas4)\n","formulas.extend(formulas5)\n","formulas.extend(formulas6)\n","\n"],"metadata":{"id":"yYCYbPWg803x","executionInfo":{"status":"ok","timestamp":1766290779031,"user_tz":-540,"elapsed":1222,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","\n","embedding_dim = embeddings.shape[1]\n","\n","print(f\"  - 샘플 수: {len(embeddings)}\")\n","print(f\"  - 임베딩 차원: {embedding_dim}\")\n","print(f\"  - Properties: {len(properties)}개\")\n","print(f\"  - 라벨 분포: 위험(1)={torch.sum(labels==1).item()}, 비위험(0)={torch.sum(labels==0).item()}\")\n","\n","# labels를 numpy로 변환\n","if isinstance(labels, torch.Tensor):\n","    labels_np = labels.numpy()\n","else:\n","    labels_np = np.array(labels)\n","\n","test_size = 0.2\n","val_size = 0.2\n","seed = 100\n","\n","# 2. 데이터 분할 (embeddings 사용)\n","indices = np.arange(len(embeddings))\n","X_train_idx, X_temp_idx, y_train, y_temp = train_test_split(\n","            indices, labels_np,\n","            test_size=test_size + val_size,\n","            random_state=seed,\n","            stratify=labels_np,\n",")\n","\n","val_size_adjusted = val_size / (test_size + val_size)\n","X_val_idx, X_test_idx, y_val, y_test = train_test_split(\n","            X_temp_idx, y_temp,\n","            test_size=1 - val_size_adjusted,\n","            random_state=seed,\n","            stratify=y_temp,\n",")\n","\n","print(\"\\n데이터 분할:\")\n","print(f\"  - Train: {len(X_train_idx)} ({100*len(X_train_idx)/len(embeddings):.1f}%)\")\n","print(f\"  - Val: {len(X_val_idx)} ({100*len(X_val_idx)/len(embeddings):.1f}%)\")\n","print(f\"  - Test: {len(X_test_idx)} ({100*len(X_test_idx)/len(embeddings):.1f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bo5kCDAnAyDP","executionInfo":{"status":"ok","timestamp":1766290781697,"user_tz":-540,"elapsed":138,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"862fd044-575c-4c1b-83a7-d0829403aa12"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["  - 샘플 수: 3008\n","  - 임베딩 차원: 36\n","  - Properties: 12개\n","  - 라벨 분포: 위험(1)=1782, 비위험(0)=1226\n","\n","데이터 분할:\n","  - Train: 1804 (60.0%)\n","  - Val: 602 (20.0%)\n","  - Test: 602 (20.0%)\n"]}]},{"cell_type":"code","source":["batch_size = 512\n","\n","\n","train_dataset = RiskDataset(\n","            embeddings=embeddings[X_train_idx],\n","            labels=y_train,\n","            use_embeddings=True\n",")\n","val_dataset = RiskDataset(\n","            embeddings=embeddings[X_val_idx],\n","            labels=y_val,\n","            use_embeddings=True\n",")\n","test_dataset = RiskDataset(\n","            embeddings=embeddings[X_test_idx],\n","            labels=y_test,\n","            use_embeddings=True\n",")\n","\n","model_input_dim = embedding_dim\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"fBVVqQJj66uc","executionInfo":{"status":"ok","timestamp":1766290784878,"user_tz":-540,"elapsed":11,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model = RiskClassifier(\n","        input_dim=model_input_dim,\n","        compute_device=str(device),\n","        use_embeddings=True,\n",")\n","\n","print(\"\\n모델 구조:\")\n","print(model)\n","# 디바이스로 이동\n","model = model.to(device)"],"metadata":{"id":"rOZ0IfZk7PA7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766290786764,"user_tz":-540,"elapsed":35,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"b4636215-9e80-490c-8028-3de35942e158"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","모델 구조:\n","RiskClassifier(\n","  (network): Sequential(\n","    (0): Linear(in_features=36, out_features=256, bias=True)\n","    (1): SiLU()\n","    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (3): Dropout(p=0, inplace=False)\n","    (4): Linear(in_features=256, out_features=128, bias=True)\n","    (5): SiLU()\n","    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (7): Dropout(p=0, inplace=False)\n","    (8): Linear(in_features=128, out_features=64, bias=True)\n","    (9): SiLU()\n","    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (11): Dropout(p=0, inplace=False)\n","    (12): Linear(in_features=64, out_features=32, bias=True)\n","    (13): SiLU()\n","    (14): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (15): Dropout(p=0, inplace=False)\n","    (16): Linear(in_features=32, out_features=2, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["#lr = 0.001\n","\n","lr = 0.01\n","\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-9)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.5, patience=2\n",")"],"metadata":{"id":"oPnyFClt7j5O","executionInfo":{"status":"ok","timestamp":1766290797395,"user_tz":-540,"elapsed":8526,"user":{"displayName":"Hyun","userId":"09823299068725153937"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"훈련 시작\")\n","print(\"=\"*60)\n","\n","best_val_f1 = 0.0\n","best_epoch = 0\n","train_history = []\n","\n","use_embeddings = True\n","epochs = 500\n","\n","hidden_dims = [256, 128, 64, 32]\n","\n","\n","save_dir = '/content/drive/MyDrive/AItom/safety_embedding_model/training_results'\n","\n","for epoch in range(epochs):\n","    # 훈련\n","    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch=epoch+1, use_embeddings=use_embeddings)\n","\n","    # 검증\n","    val_metrics = evaluate(model, val_loader, criterion, device, use_embeddings=use_embeddings)\n","\n","    # 학습률 스케줄러 업데이트\n","    scheduler.step(val_metrics['loss'])\n","\n","    # 기록\n","    train_history.append({\n","            'epoch': epoch + 1,\n","            'train_loss': train_loss,\n","            'train_acc': train_acc,\n","            'val_loss': val_metrics['loss'],\n","            'val_acc': val_metrics['accuracy'],\n","            'val_f1': val_metrics['f1'],\n","            'val_auc': val_metrics['auc'],\n","    })\n","\n","    # 최고 성능 모델 저장\n","    if val_metrics['f1'] > best_val_f1:\n","        best_val_f1 = val_metrics['f1']\n","        best_epoch = epoch + 1\n","\n","        # 모델 저장\n","        os.makedirs(save_dir, exist_ok=True)\n","        torch.save({\n","                'epoch': epoch + 1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_f1': best_val_f1,\n","                'properties': DEFAULT_PROPERTIES,\n","                'hidden_dims': hidden_dims,\n","        }, os.path.join(save_dir, 'best_model6.pth'))\n","\n","    # 진행 상황 출력\n","    if (epoch + 1) % 5 == 0 or epoch == 0:\n","        print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n","                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n","                  f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | \"\n","                  f\"Val F1: {val_metrics['f1']:.4f} | Val AUC: {val_metrics['auc']:.4f}\")\n","\n","print(\"\\n최고 성능: Epoch {best_epoch}, Val F1: {best_val_f1:.4f}\")\n"],"metadata":{"id":"yoGkuO4878Vj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766290871541,"user_tz":-540,"elapsed":72102,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"5f4f06d8-e155-40e8-e772-8cc7274ca615"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","훈련 시작\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch   1/500 | Train Loss: 0.7025 | Train Acc: 0.5815 | Val Loss: 0.7032 | Val Acc: 0.4884 | Val F1: 0.2524 | Val AUC: 0.6895\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch   5/500 | Train Loss: 0.5046 | Train Acc: 0.7605 | Val Loss: 0.6116 | Val Acc: 0.6827 | Val F1: 0.7766 | Val AUC: 0.7702\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  10/500 | Train Loss: 0.4426 | Train Acc: 0.7894 | Val Loss: 0.7363 | Val Acc: 0.6312 | Val F1: 0.7477 | Val AUC: 0.7115\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  15/500 | Train Loss: 0.3688 | Train Acc: 0.8331 | Val Loss: 0.5206 | Val Acc: 0.7508 | Val F1: 0.8021 | Val AUC: 0.8063\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  20/500 | Train Loss: 0.3235 | Train Acc: 0.8537 | Val Loss: 0.5776 | Val Acc: 0.7591 | Val F1: 0.8033 | Val AUC: 0.7981\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  25/500 | Train Loss: 0.2982 | Train Acc: 0.8614 | Val Loss: 0.5925 | Val Acc: 0.7641 | Val F1: 0.8006 | Val AUC: 0.8138\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  30/500 | Train Loss: 0.2901 | Train Acc: 0.8692 | Val Loss: 0.6022 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  35/500 | Train Loss: 0.2878 | Train Acc: 0.8731 | Val Loss: 0.6117 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  40/500 | Train Loss: 0.2859 | Train Acc: 0.8708 | Val Loss: 0.6147 | Val Acc: 0.7658 | Val F1: 0.8017 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  45/500 | Train Loss: 0.2869 | Train Acc: 0.8736 | Val Loss: 0.6167 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  50/500 | Train Loss: 0.2894 | Train Acc: 0.8670 | Val Loss: 0.6152 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  55/500 | Train Loss: 0.2892 | Train Acc: 0.8742 | Val Loss: 0.6151 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  60/500 | Train Loss: 0.2874 | Train Acc: 0.8742 | Val Loss: 0.6128 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  65/500 | Train Loss: 0.3024 | Train Acc: 0.8708 | Val Loss: 0.6147 | Val Acc: 0.7625 | Val F1: 0.7989 | Val AUC: 0.8096\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  70/500 | Train Loss: 0.2872 | Train Acc: 0.8725 | Val Loss: 0.6140 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  75/500 | Train Loss: 0.2891 | Train Acc: 0.8725 | Val Loss: 0.6141 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8096\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  80/500 | Train Loss: 0.2897 | Train Acc: 0.8720 | Val Loss: 0.6137 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  85/500 | Train Loss: 0.2944 | Train Acc: 0.8714 | Val Loss: 0.6180 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8085\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  90/500 | Train Loss: 0.2993 | Train Acc: 0.8686 | Val Loss: 0.6152 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch  95/500 | Train Loss: 0.2934 | Train Acc: 0.8675 | Val Loss: 0.6165 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 100/500 | Train Loss: 0.2860 | Train Acc: 0.8697 | Val Loss: 0.6160 | Val Acc: 0.7625 | Val F1: 0.7977 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 105/500 | Train Loss: 0.2905 | Train Acc: 0.8653 | Val Loss: 0.6163 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8086\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 110/500 | Train Loss: 0.2932 | Train Acc: 0.8720 | Val Loss: 0.6154 | Val Acc: 0.7625 | Val F1: 0.7977 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 115/500 | Train Loss: 0.2878 | Train Acc: 0.8642 | Val Loss: 0.6132 | Val Acc: 0.7674 | Val F1: 0.8017 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 120/500 | Train Loss: 0.2899 | Train Acc: 0.8725 | Val Loss: 0.6170 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 125/500 | Train Loss: 0.2880 | Train Acc: 0.8764 | Val Loss: 0.6147 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 130/500 | Train Loss: 0.2830 | Train Acc: 0.8742 | Val Loss: 0.6126 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 135/500 | Train Loss: 0.2902 | Train Acc: 0.8747 | Val Loss: 0.6145 | Val Acc: 0.7641 | Val F1: 0.8006 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 140/500 | Train Loss: 0.2837 | Train Acc: 0.8747 | Val Loss: 0.6156 | Val Acc: 0.7674 | Val F1: 0.8017 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 145/500 | Train Loss: 0.2893 | Train Acc: 0.8736 | Val Loss: 0.6157 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 150/500 | Train Loss: 0.2908 | Train Acc: 0.8703 | Val Loss: 0.6169 | Val Acc: 0.7658 | Val F1: 0.7994 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 155/500 | Train Loss: 0.2873 | Train Acc: 0.8686 | Val Loss: 0.6125 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 160/500 | Train Loss: 0.2922 | Train Acc: 0.8664 | Val Loss: 0.6157 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 165/500 | Train Loss: 0.2806 | Train Acc: 0.8753 | Val Loss: 0.6149 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 170/500 | Train Loss: 0.2859 | Train Acc: 0.8731 | Val Loss: 0.6156 | Val Acc: 0.7625 | Val F1: 0.7977 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 175/500 | Train Loss: 0.2869 | Train Acc: 0.8708 | Val Loss: 0.6148 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 180/500 | Train Loss: 0.2914 | Train Acc: 0.8664 | Val Loss: 0.6188 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 185/500 | Train Loss: 0.2810 | Train Acc: 0.8720 | Val Loss: 0.6146 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 190/500 | Train Loss: 0.2859 | Train Acc: 0.8703 | Val Loss: 0.6130 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 195/500 | Train Loss: 0.2893 | Train Acc: 0.8714 | Val Loss: 0.6159 | Val Acc: 0.7691 | Val F1: 0.8034 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 200/500 | Train Loss: 0.2900 | Train Acc: 0.8697 | Val Loss: 0.6154 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 205/500 | Train Loss: 0.2875 | Train Acc: 0.8681 | Val Loss: 0.6128 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 210/500 | Train Loss: 0.2859 | Train Acc: 0.8714 | Val Loss: 0.6138 | Val Acc: 0.7674 | Val F1: 0.8017 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 215/500 | Train Loss: 0.2907 | Train Acc: 0.8714 | Val Loss: 0.6154 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 220/500 | Train Loss: 0.2866 | Train Acc: 0.8697 | Val Loss: 0.6135 | Val Acc: 0.7608 | Val F1: 0.7972 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 225/500 | Train Loss: 0.2884 | Train Acc: 0.8686 | Val Loss: 0.6141 | Val Acc: 0.7674 | Val F1: 0.8011 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 230/500 | Train Loss: 0.2896 | Train Acc: 0.8708 | Val Loss: 0.6144 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 235/500 | Train Loss: 0.2870 | Train Acc: 0.8725 | Val Loss: 0.6159 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 240/500 | Train Loss: 0.2879 | Train Acc: 0.8708 | Val Loss: 0.6162 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 245/500 | Train Loss: 0.2907 | Train Acc: 0.8659 | Val Loss: 0.6137 | Val Acc: 0.7691 | Val F1: 0.8028 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 250/500 | Train Loss: 0.2923 | Train Acc: 0.8747 | Val Loss: 0.6147 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 255/500 | Train Loss: 0.2912 | Train Acc: 0.8681 | Val Loss: 0.6140 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 260/500 | Train Loss: 0.2846 | Train Acc: 0.8736 | Val Loss: 0.6167 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 265/500 | Train Loss: 0.2884 | Train Acc: 0.8714 | Val Loss: 0.6127 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8096\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 270/500 | Train Loss: 0.2910 | Train Acc: 0.8736 | Val Loss: 0.6151 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8094\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 275/500 | Train Loss: 0.2830 | Train Acc: 0.8753 | Val Loss: 0.6164 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 280/500 | Train Loss: 0.2915 | Train Acc: 0.8659 | Val Loss: 0.6156 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 285/500 | Train Loss: 0.2923 | Train Acc: 0.8697 | Val Loss: 0.6144 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8094\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 290/500 | Train Loss: 0.2903 | Train Acc: 0.8725 | Val Loss: 0.6134 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 295/500 | Train Loss: 0.2866 | Train Acc: 0.8742 | Val Loss: 0.6134 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8099\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 300/500 | Train Loss: 0.2942 | Train Acc: 0.8731 | Val Loss: 0.6152 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 305/500 | Train Loss: 0.2936 | Train Acc: 0.8647 | Val Loss: 0.6136 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8094\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 310/500 | Train Loss: 0.2984 | Train Acc: 0.8686 | Val Loss: 0.6164 | Val Acc: 0.7625 | Val F1: 0.7989 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 315/500 | Train Loss: 0.2865 | Train Acc: 0.8747 | Val Loss: 0.6164 | Val Acc: 0.7658 | Val F1: 0.8017 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 320/500 | Train Loss: 0.2950 | Train Acc: 0.8659 | Val Loss: 0.6145 | Val Acc: 0.7641 | Val F1: 0.8006 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 325/500 | Train Loss: 0.2847 | Train Acc: 0.8720 | Val Loss: 0.6168 | Val Acc: 0.7674 | Val F1: 0.8028 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 330/500 | Train Loss: 0.2889 | Train Acc: 0.8720 | Val Loss: 0.6125 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 335/500 | Train Loss: 0.2948 | Train Acc: 0.8714 | Val Loss: 0.6156 | Val Acc: 0.7674 | Val F1: 0.8028 | Val AUC: 0.8092\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 340/500 | Train Loss: 0.2904 | Train Acc: 0.8742 | Val Loss: 0.6168 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 345/500 | Train Loss: 0.2936 | Train Acc: 0.8708 | Val Loss: 0.6157 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 350/500 | Train Loss: 0.2836 | Train Acc: 0.8747 | Val Loss: 0.6163 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 355/500 | Train Loss: 0.2828 | Train Acc: 0.8753 | Val Loss: 0.6155 | Val Acc: 0.7641 | Val F1: 0.7989 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 360/500 | Train Loss: 0.2939 | Train Acc: 0.8714 | Val Loss: 0.6158 | Val Acc: 0.7674 | Val F1: 0.8023 | Val AUC: 0.8094\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 365/500 | Train Loss: 0.2946 | Train Acc: 0.8714 | Val Loss: 0.6165 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 370/500 | Train Loss: 0.2864 | Train Acc: 0.8725 | Val Loss: 0.6163 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 375/500 | Train Loss: 0.2898 | Train Acc: 0.8720 | Val Loss: 0.6160 | Val Acc: 0.7625 | Val F1: 0.7989 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 380/500 | Train Loss: 0.2870 | Train Acc: 0.8636 | Val Loss: 0.6153 | Val Acc: 0.7625 | Val F1: 0.7989 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 385/500 | Train Loss: 0.2876 | Train Acc: 0.8758 | Val Loss: 0.6176 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 390/500 | Train Loss: 0.2962 | Train Acc: 0.8725 | Val Loss: 0.6177 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 395/500 | Train Loss: 0.2800 | Train Acc: 0.8708 | Val Loss: 0.6125 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 400/500 | Train Loss: 0.2809 | Train Acc: 0.8697 | Val Loss: 0.6118 | Val Acc: 0.7708 | Val F1: 0.8051 | Val AUC: 0.8102\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 405/500 | Train Loss: 0.2895 | Train Acc: 0.8736 | Val Loss: 0.6161 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8085\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 410/500 | Train Loss: 0.2906 | Train Acc: 0.8703 | Val Loss: 0.6144 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 415/500 | Train Loss: 0.2905 | Train Acc: 0.8681 | Val Loss: 0.6156 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 420/500 | Train Loss: 0.2855 | Train Acc: 0.8736 | Val Loss: 0.6160 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 425/500 | Train Loss: 0.2861 | Train Acc: 0.8731 | Val Loss: 0.6165 | Val Acc: 0.7641 | Val F1: 0.7983 | Val AUC: 0.8091\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 430/500 | Train Loss: 0.2931 | Train Acc: 0.8692 | Val Loss: 0.6165 | Val Acc: 0.7625 | Val F1: 0.7977 | Val AUC: 0.8087\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 435/500 | Train Loss: 0.2848 | Train Acc: 0.8742 | Val Loss: 0.6147 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8088\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 440/500 | Train Loss: 0.2898 | Train Acc: 0.8720 | Val Loss: 0.6160 | Val Acc: 0.7641 | Val F1: 0.7983 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 445/500 | Train Loss: 0.2893 | Train Acc: 0.8780 | Val Loss: 0.6153 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8094\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 450/500 | Train Loss: 0.2880 | Train Acc: 0.8692 | Val Loss: 0.6180 | Val Acc: 0.7625 | Val F1: 0.7983 | Val AUC: 0.8083\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 455/500 | Train Loss: 0.2909 | Train Acc: 0.8720 | Val Loss: 0.6165 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8097\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 460/500 | Train Loss: 0.2944 | Train Acc: 0.8708 | Val Loss: 0.6141 | Val Acc: 0.7658 | Val F1: 0.8006 | Val AUC: 0.8098\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 465/500 | Train Loss: 0.2882 | Train Acc: 0.8647 | Val Loss: 0.6166 | Val Acc: 0.7641 | Val F1: 0.7994 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 470/500 | Train Loss: 0.2866 | Train Acc: 0.8681 | Val Loss: 0.6150 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 475/500 | Train Loss: 0.2909 | Train Acc: 0.8703 | Val Loss: 0.6135 | Val Acc: 0.7641 | Val F1: 0.8000 | Val AUC: 0.8089\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 480/500 | Train Loss: 0.2911 | Train Acc: 0.8731 | Val Loss: 0.6140 | Val Acc: 0.7641 | Val F1: 0.7989 | Val AUC: 0.8095\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 485/500 | Train Loss: 0.2855 | Train Acc: 0.8675 | Val Loss: 0.6118 | Val Acc: 0.7674 | Val F1: 0.8017 | Val AUC: 0.8096\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 490/500 | Train Loss: 0.2862 | Train Acc: 0.8731 | Val Loss: 0.6158 | Val Acc: 0.7658 | Val F1: 0.8000 | Val AUC: 0.8093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 495/500 | Train Loss: 0.2953 | Train Acc: 0.8647 | Val Loss: 0.6140 | Val Acc: 0.7658 | Val F1: 0.8011 | Val AUC: 0.8090\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 500/500 | Train Loss: 0.2869 | Train Acc: 0.8703 | Val Loss: 0.6136 | Val Acc: 0.7691 | Val F1: 0.8039 | Val AUC: 0.8096\n","\n","최고 성능: Epoch {best_epoch}, Val F1: {best_val_f1:.4f}\n"]}]},{"cell_type":"code","source":["\n","print(\"\\n\" + \"=\"*60)\n","print(\"테스트 세트 평가\")\n","print(\"=\"*60)\n","\n","# 최고 모델 로드\n","checkpoint = torch.load(os.path.join(save_dir, 'best_model6.pth'))\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","test_metrics = evaluate(model, test_loader, criterion, device, use_embeddings=use_embeddings)\n","\n","print(\"\\n테스트 세트 성능:\")\n","print(f\"  - Accuracy: {test_metrics['accuracy']:.4f}\")\n","print(f\"  - Precision: {test_metrics['precision']:.4f}\")\n","print(f\"  - Recall: {test_metrics['recall']:.4f}\")\n","print(f\"  - F1 Score: {test_metrics['f1']:.4f}\")\n","print(f\"  - AUC-ROC: {test_metrics['auc']:.4f}\")\n","\n","# Confusion Matrix\n","cm = confusion_matrix(test_metrics['labels'], test_metrics['predictions'])\n","print(\"\\nConfusion Matrix:\")\n","print(\"                Predicted\")\n","print(\"              Safe  Risk\")\n","print(f\"Actual Safe    {cm[0,0]:4d}  {cm[0,1]:4d}\")\n","print(f\"       Risk    {cm[1,0]:4d}  {cm[1,1]:4d}\")\n","\n","# Classification Report\n","print(\"\\nClassification Report:\")\n","print(classification_report(\n","        test_metrics['labels'],\n","        test_metrics['predictions'],\n","        target_names=['Safe', 'Risk']\n","))\n","\n","# 히스토리 저장\n","history_df = pd.DataFrame(train_history)\n","history_df.to_csv(os.path.join(save_dir, 'training_history.csv'), index=False)\n","print(f\"\\n훈련 히스토리 저장: {os.path.join(save_dir, 'training_history.csv')}\")\n","\n","print(\"\\n훈련 완료!\")"],"metadata":{"id":"5aJn0dDW9Ye2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766290873397,"user_tz":-540,"elapsed":448,"user":{"displayName":"Hyun","userId":"09823299068725153937"}},"outputId":"8a3946fd-4d91-4849-c29a-5ed8999eb864"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","테스트 세트 평가\n","============================================================\n","\n","테스트 세트 성능:\n","  - Accuracy: 0.8056\n","  - Precision: 0.8504\n","  - Recall: 0.8146\n","  - F1 Score: 0.8321\n","  - AUC-ROC: 0.8609\n","\n","Confusion Matrix:\n","                Predicted\n","              Safe  Risk\n","Actual Safe     195    51\n","       Risk      66   290\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        Safe       0.75      0.79      0.77       246\n","        Risk       0.85      0.81      0.83       356\n","\n","    accuracy                           0.81       602\n","   macro avg       0.80      0.80      0.80       602\n","weighted avg       0.81      0.81      0.81       602\n","\n","\n","훈련 히스토리 저장: /content/drive/MyDrive/AItom/safety_embedding_model/training_results/training_history.csv\n","\n","훈련 완료!\n"]}]}]}